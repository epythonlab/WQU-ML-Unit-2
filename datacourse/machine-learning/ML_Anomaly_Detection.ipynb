{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/ML_Anomaly_Detection.py append\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "<!-- requirement: images/ocsvm.png -->\n",
    "<!-- requirement: data/energy_data.csv -->\n",
    "\n",
    "Often times, we need to find abnormal and unusual values in our data set. This process is referred to as **anomaly detection**. Identifying these data points can serve multiple purposes, such as\n",
    "\n",
    "1. Removing outliers in a training set before fitting a machine learning model.\n",
    "1. Analyzing a set of observations to identify whether something is wrong with the process. For example, are there any anomalous server logs that _may_ indicate a security breach.\n",
    "\n",
    "The application of distinguishing whether something is abnormal or not falls into two divisions, **outlier** or **novelty detection**. These two terms are often lumped together but do have distinct definitions.\n",
    "\n",
    "* **Outlier Detection:** The process of identifying observations that deviate substantially from the rest. Outlier detection models are trained with data sets that include outliers; it is not \"clean\". The model learns how much a point can deviate to be classified as an outlier.\n",
    "\n",
    "* **Novelty Detection:** The process of identifying novel points by training a model with a data set that is not \"polluted\" with outliers. The model learns a boundary, or boundaries, the encompasses all normal/regular points. Any points that reside outside of these boundaries are new and thus novel.\n",
    "\n",
    "The distinction is subtle but certain algorithms are referred to as either outlier or novelty detection. However, in practice, both classes can work well regardless if the application is purely novelty or outlier detection. A further discussion of novelty versus outlier detection can be read [here](https://scikit-learn.org/stable/modules/outlier_detection.html). Anomaly detection is _usually_ an unsupervised machine learning technique because rarely do we have labels for the observations. As such, the algorithms for detecting anomalies will rely purely on features of the observations.\n",
    "\n",
    "In this notebook, we will go over two popular algorithms for outlier and novelty detection before working on a case study using time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in `scikit-learn`\n",
    "\n",
    "In `scikit-learn`, anomaly detection algorithms are unsupervised learners. We will discuss two models for anomaly detection in `scikit-learn`, **one-class SVM** and **isolation forest**. Both are unsupervised learning models with a similar interface; the two key methods are:\n",
    "\n",
    "* `fit(X)`: fit/train the model with data set `X`.\n",
    "* `predict(X)`: determine whether the observations in `X` are inliers `1` or outliers `-1`.\n",
    "* `decision_function(X)`: score/metric used to determine whether a point is an inlier/outlier.\n",
    "\n",
    "Note, the interpretation of the output of the `decision_function` method is algorithm specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class Support Vector Machines\n",
    "\n",
    "The support vector machine classifier can be tweaked to serve novelty detection applications, referred to as one-class support vector machines. Consider a binary classification situation where all points in the training set belong to the same class, hence the name of the algorithm. Because all training points are in the same class, it is assumed that there are no outliers. The one-class SVM is a novelty detector. \n",
    "\n",
    "The points are transformed to a higher dimensional space where you have the freedom to locate the origin of the coordinates. The algorithm's task becomes to locate a hyperplane in the space that best separates the data from the origin. The catch is that hyperplane must go through the origin and located on the origin is the only member of the second class. The algorithm works by pushing as many of the vectors in the training set away from the origin in the feature space. As before, the model includes slack variable for vectors that violate the margin. The algorithm is prevented from pushing the vectors infinitely far away from the origin as the single member of the second class always resides on the hyperplane, incurring a large penalty when the origin is very far away from most of the training points. The algorithm has to find the best balance between origin separation and margin violations from the training set. The image below illustrates an example of the algorithm. For visual purposes, we have only used two dimensions but in practice the algorithm works in a large dimensional space to achieve better separation.\n",
    "\n",
    "<img src=\"images/ocsvm.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The governing equation and constraints are\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\zeta, \\rho} \\frac{1}{2} \\|\\beta\\|^2 + \\frac{1}{\\nu n}\\sum_{j=1}^n \\zeta_j - \\rho \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " h(x_{j\\cdot}) \\cdot \\beta \\ge \\rho -\\zeta_j &  \\\\\n",
    " \\zeta_j \\ge 0 ,\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "where $h(x_j)$ is a kernel function, $\\rho$ is distance from the origin to the hyperplane, and $\\nu$ is a hyperparameter that is the upper bound for the fraction of training error and the lower bound for the fraction of support vectors. Notice how the constraint is forcing points to be at least $\\rho$ away from the margin, lest it incurs a margin violation as $\\zeta$ must be set large enough to satisfy the inequality.\n",
    "\n",
    "Let's use the one-class SVM to see how it works. We will be using the wine data set provided by `scikit-learn`. The details of the data set are not important, only that it has 178 observations with 13 numerical features.\n",
    "\n",
    "**Question**\n",
    "* What technique should we use on the wine data set to easily visualize our analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load data set\n",
    "data = load_wine()\n",
    "X = data['data']\n",
    "\n",
    "# truncate to two variables\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('dim_red', PCA(n_components=2))])\n",
    "Xt = pipe.fit_transform(X)\n",
    "\n",
    "# generate novel/outlier points\n",
    "np.random.seed(1)\n",
    "theta = 2*np.pi*np.random.random(10)\n",
    "X_test = np.vstack((4*np.cos(theta) + np.random.random(10), 4*np.sin(theta) + np.random.random(10)))\n",
    "\n",
    "plt.scatter(*Xt.T)\n",
    "plt.scatter(*X_test, c='red')\n",
    "plt.xlabel('$\\\\xi_1$')\n",
    "plt.ylabel('$\\\\xi_2$');\n",
    "plt.legend([\"training set\", \"novel/outliers\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization below plots the data along with the boundaries that determine whether a point is consider novel or not. The filled contour lines in the plot represent values of the decision function of the one-class SVM. The `decision_function` method reports the signed distance (negative means on the wrong side) between the point and the hyperplane. The visualization allows you to modify $\\nu$, the upper bound for the false positive rate. You can also consider $\\nu$ as the probability of having a new but regular observation outside the region defining regular points. As $\\nu$ decreases, the area encompassing the regular points increases. As with the standard kernelized SVM, you can change the kernel function, but `rbf` usually works the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def plot_one_class_svm(X, X_test):\n",
    "    def plotter(nu=0.95):\n",
    "        clf = OneClassSVM(nu=nu, gamma='auto')\n",
    "        clf.fit(X)\n",
    "        y_pred = clf.predict(X)\n",
    "        fp_rate = (y_pred == -1).sum()/len(X)\n",
    "        \n",
    "        X1, X2 = np.meshgrid(np.linspace(-5, 5), np.linspace(-5, 5))\n",
    "        y_proba = clf.decision_function(np.hstack((X1.reshape(-1, 1), X2.reshape(-1, 1))))\n",
    "        Z = y_proba.reshape(50, 50)\n",
    "        \n",
    "        fig = plt.figure(figsize=(8, 5), facecolor='w', edgecolor='k')\n",
    "        plt.contourf(X1, X2, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\n",
    "        plt.colorbar()\n",
    "        a = plt.contour(X1, X2, Z, levels=[0], linewidths=2, colors='black')            \n",
    "        b1 = plt.scatter(*X.T, c='blue')\n",
    "        b2 = plt.scatter(*X_test, c='red')\n",
    "        plt.title(\"false positive rate: {:g}\".format(fp_rate))\n",
    "        plt.legend([a.collections[0], b1, b2], [\"boundary\", \" true inliers\", \"true outliers\"], frameon=True, \n",
    "                   loc=\"lower left\")\n",
    "    return plotter\n",
    "\n",
    "nu_slider = FloatSlider(min=0.01, max=0.99, step=0.01, value=0.5, description='$\\\\nu$')\n",
    "interact(plot_one_class_svm(Xt, X_test), nu=nu_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest\n",
    "\n",
    "Isolation forests is an outlier detection algorithm that uses decision trees. The principle isolation forest works on is that outliers are points with features that are considerably different than the rest of the data, the inliers. Consider a data set in a $p$-dimensional space. The inliers will be closer together while the outliers will be farther apart. As we have seen, decision trees divide up a $p$-dimensional space using orthogonal cuts. \n",
    "\n",
    "Consider a decision tree that is constructed by making random cuts with randomly chosen features. The tree is allowed to grow until all points have been isolated. It will be easier to isolate or box in the outliers than the inliers. In other words, less splits are required to isolate outliers compared to the inliers and outlier nodes will reside closer to the root node. The process of constructing a tree with random cuts is repeated to create an ensemble, hence the term forest in the name of the algorithm. For each data point, the average path/splits required to isolate the point is a metric for the regularity or normality of the point. While the algorithm could have adopted a more sophisticated manner to isolate points, making random splits is computationally cheap and averaging across all trees considers the multiple manners to isolate the data. The two key hyperparameters are\n",
    "\n",
    "* `n_estimators`: The number of trees to use in the ensemble.\n",
    "* `contamination`: The fraction of outliers in the data set.\n",
    "\n",
    "The `decision_function` method returns a score for a set of observations, a negative or positive score means the observation is labeled as an outlier or inlier, respectively. This score is related to the path length, number of splits, to isolate each observation, averaged across all trees in the forest, but with an offset,\n",
    "\n",
    "$$\n",
    "\\text{score} = \\text{mean path length} - \\text{offset},\n",
    "$$\n",
    "\n",
    "where the offset is chosen based on the set contamination level. For example, if the contamination fraction was set to 0.2, then the offset if chosen such that 20% of the training data have a negative score. Let's visualize the result of using an isolation forest on the wine data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def plot_isolation_forest(X, X_test):\n",
    "    def plotter(contamination=0.2):\n",
    "        clf = IsolationForest(n_estimators=100, contamination=contamination, behaviour='new')\n",
    "        clf.fit(X)\n",
    "    \n",
    "        y_pred = clf.predict(X)\n",
    "        outlier_rate = (y_pred == -1).sum()/len(X)\n",
    "        \n",
    "        X1, X2 = np.meshgrid(np.linspace(-5, 5), np.linspace(-5, 5))\n",
    "        y_proba = clf.decision_function(np.hstack((X1.reshape(-1, 1), X2.reshape(-1, 1))))\n",
    "        Z = y_proba.reshape(50, 50)\n",
    "        \n",
    "        fig = plt.figure(figsize=(8, 5), facecolor='w', edgecolor='k')\n",
    "        plt.contourf(X1, X2, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\n",
    "        plt.colorbar()\n",
    "        a = plt.contour(X1, X2, Z, levels=[0], linewidths=2, colors='black')            \n",
    "        b1 = plt.scatter(*X.T, c='blue')\n",
    "        b2 = plt.scatter(*X_test, c='red')\n",
    "        plt.title(\"outlier fraction: {:g}\".format(outlier_rate))\n",
    "        plt.legend([a.collections[0], b1, b2], [\"boundary\", \" true inliers\", \"true outliers\"], frameon=True, \n",
    "                   loc=\"lower left\")    \n",
    "    return plotter\n",
    "\n",
    "cont_slider = FloatSlider(min=0.01, max=0.5, value=0.1, step=0.01, description=\"fraction\")\n",
    "interact(plot_isolation_forest(Xt, X_test), contamination=cont_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "* Which algorithm, isolation forest or one-class SVM, has a better training time complexity? What influenced your decision?\n",
    "* What advantages of decision trees in general would still be present in the isolation forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of one-class SVM and isolation forest\n",
    "\n",
    "Here are a few things to be aware between one-class SVM and isolation forest.\n",
    "\n",
    "* Both algorithms are capable of properly modeling multi-modal data sets.\n",
    "* One class SVM is sensitive to outliers, making it more appropriate for novelty detection, when the training data is not contaminated with outliers.\n",
    "* Since the splits of the decision tree are chosen at random, isolation forest is faster to train.\n",
    "* In general, SVM are slow to train, especially with respect to the training set size.\n",
    "\n",
    "Additionally, the two methods inherit the pros and cons of their parent algorithm. There are other outlier and novelty detection algorithms available in `scikit-learn` and a comparison and overview of other methods are outlined [here](https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html#sphx-glr-auto-examples-plot-anomaly-comparison-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection in a time series\n",
    "\n",
    "Anomaly detection can be applied to a time series where we want to create a baseline model and determine the deviation of the observations with the baseline. If the deviation is large enough, the observation is deemed anomalous and is flagged. In general, novelty and outlier detection does not tell us _why_ something is possibility an outlier, the conditions and causes that led to an unusual observation. For example, if we are observing server logs, anomalous observations may be a result of some equipment or code breakdown or something malignant like a security breach.\n",
    "\n",
    "In this case study, we analyze appliance energy use for a 4.5 month time period. Data was collected at a sampling rate of 10 minutes. Given the large variability in energy usage at a sampling rate of 10 minutes, we will resample the time series at an hourly interval. More of the data set can be learned [here](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/energy_data.csv\", parse_dates=[\"date\"])              \n",
    "df = df.set_index('date')\n",
    "df_hourly = df.resample(\"H\").mean() # resample hourly\n",
    "\n",
    "energy = df_hourly['Appliances']\n",
    "energy.plot()\n",
    "plt.ylabel(\"energy (Wh)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* What should our plan of attack be for analyzing the time series?\n",
    "* What do you observe in the time series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Analysis\n",
    "\n",
    "While it is hard to tell, there are periodic behaviors in the time series. We can better spotlight the dominant frequencies that support the time series using Fourier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "\n",
    "sampling_rate = (energy.index[1] - energy.index[0]).total_seconds()\n",
    "sampling_rate = sampling_rate / (60 * 60 * 24) # day\n",
    "\n",
    "Y = fftpack.fft(energy - energy.mean())\n",
    "freq = np.linspace(0, 1/sampling_rate, len(Y))\n",
    "\n",
    "plt.plot(freq[:len(freq)//2], np.abs(Y[:len(Y)//2]))\n",
    "plt.xlabel(\"cycles per day\")\n",
    "plt.ylabel(\"Fourier transform\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series has four dominant frequencies: daily, twice-daily, three times a day, and four times a day. In other words, 6, 8, 12, and 24 hour periods. Given our everyday experience, we probably would have anticipated these frequencies/periods but it is reassuring that they can be revealed via Fourier analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating day of the week\n",
    "\n",
    "With time series data, a commonly generated feature is the day of the week for the observations. It might be tempting to capture day of the week behavior using a sinusoidal component but let's analyze the energy usage for each day of the week to understand how to best model it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_of_week = pd.DataFrame({'day': energy.index.dayofweek, 'count': energy.values})\n",
    "grouped_by_day = df_day_of_week.groupby('day')\n",
    "\n",
    "grouped_by_day.mean().plot(kind='bar')\n",
    "plt.xticks(range(7), ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy usage as a function of the day of the week fluctuates but is not periodic; it is best captured using one hot encoded features. We will include one hot encoded features for each day of the week to our baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial baseline model\n",
    "\n",
    "We will need to create some custom transformers to work with our pandas times series data. Specifically, we need a transformer to extract out the indices, create Fourier components, and transform `datetime` objects into a unit of time. A common reference point used to translate a date into a unit of time is [Unix time](https://en.wikipedia.org/wiki/Unix_time). It is defined as the time since 00:00:00 Thursday, 1 January 1970 Coordinated Universal Time (UTC), minus leap seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class IndexSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Return indices of a data frame for use in other estimators.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        return df.index\n",
    "\n",
    "class FourierComponents(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, freqs):\n",
    "        \"\"\"Create features based on sin(2*pi*f*t) and cos(2*pi*f*t).\"\"\"\n",
    "        self.freqs = freqs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xt = np.zeros((X.shape[0], 2 * len(self.freqs)))\n",
    "        t_0 = X[0]\n",
    "        for i, f in enumerate(self.freqs):\n",
    "            Xt[:, 2 * i] = np.cos(2 * np.pi * f * (X)).reshape(-1)\n",
    "            Xt[:, 2 * i + 1] = np.sin(2 * np.pi * f * (X)).reshape(-1)\n",
    "\n",
    "        return Xt\n",
    "\n",
    "class EpochTime(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, unit):\n",
    "        \"\"\"Transform datetime object to some unit of time since the start of the epoch.\"\"\"\n",
    "        self.unit = unit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        epoch_time = np.array([x.value for x in X])\n",
    "\n",
    "        if self.unit == \"seconds\":\n",
    "            return epoch_time / (1000000000)\n",
    "        elif self.unit == \"minutes\":\n",
    "            return epoch_time / (1000000000) / 60\n",
    "        elif self.unit == \"hours\":\n",
    "            return epoch_time / (1000000000) / 60 / 60\n",
    "        else:\n",
    "            return epoch_time\n",
    "        \n",
    "class DayOfWeek(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Determine the day of the week for datetime objects.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([x.dayofweek for x in X]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional useful functions for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(df, cutoff, target):\n",
    "    \"\"\"Perform a train/test split on a data frame based on a cutoff date.\"\"\"\n",
    "    \n",
    "    ind = df.index < cutoff\n",
    "    \n",
    "    df_train = df.loc[ind]\n",
    "    df_test = df.loc[~ind]\n",
    "    y_train = df.loc[ind, target]\n",
    "    y_test = df.loc[~ind, target]\n",
    "    \n",
    "    return df_train, df_test, y_train, y_test\n",
    "\n",
    "def plot_results(df, y_pred):\n",
    "    \"\"\"Plot predicted results and residuals.\"\"\"\n",
    "    \n",
    "    plt.plot(df.index, y_pred, '-r')\n",
    "    energy.plot()\n",
    "    plt.ylabel('energy (Wh)')\n",
    "    plt.legend(['true', 'predicted'])\n",
    "    plt.show();\n",
    "\n",
    "    plt.plot(resd)\n",
    "    plt.ylabel('residual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# perform train/test split\n",
    "cutoff = \"Mar-2016\" # roughly corresponding to 10% of the data\n",
    "df_train, df_test, y_train, y_test = ts_train_test_split(df_hourly, cutoff, 'Appliances')\n",
    "\n",
    "# construct and train model\n",
    "freqs = np.array([1, 2, 3]) / 24 / 60 # 24, 12, and 8 hour periods\n",
    "selector = IndexSelector()\n",
    "epoch_time = EpochTime(\"minutes\")\n",
    "fourier_components = FourierComponents(freqs)\n",
    "one_hot = OneHotEncoder(sparse=False, categories='auto')\n",
    "lr = LinearRegression()\n",
    "\n",
    "fourier = Pipeline([(\"time\", epoch_time),\n",
    "                    (\"sine_cosine\", fourier_components)])\n",
    "day_of_week = Pipeline([(\"day\", DayOfWeek()),\n",
    "                        (\"encoder\", one_hot)])\n",
    "union = FeatureUnion([(\"fourier\", fourier),\n",
    "                      (\"day_of_week\", day_of_week)])\n",
    "\n",
    "pipe = Pipeline([(\"indices\", selector),\n",
    "                 (\"union\", union),\n",
    "                 (\"regressor\", lr)])\n",
    "pipe.fit(df_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = pipe.predict(df_hourly)\n",
    "resd = energy - y_pred\n",
    "print(\"Test set R^2: {:g}\".format(pipe.score(df_test, y_test)))\n",
    "plot_results(df_hourly, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very apparent that the initial baseline model is not adequate for the time series. The residuals reveal:\n",
    "\n",
    "1. No long term trends.\n",
    "1. The time series has a lot of shock events, large increases in energy use, probably as a result of sudden and short use of an appliance.\n",
    "\n",
    "Let's next analyze the residuals for any temporal correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise based features\n",
    "\n",
    "The first thing we want to unveil is the correlation of past residuals with current values. An autocorrelation plot will inform us of the characteristic time scale of the process to guide us when generating noise based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(resd)\n",
    "plt.xlabel('Lag (hour)')\n",
    "plt.xlim([0, 50]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time scale appears to be anywhere from 10 to 20 hours. Let's incorporate window based features from our residuals to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin\n",
    "\n",
    "class ResidualFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window=100):\n",
    "        \"\"\"Generate features based on window statistics of past noise/residuals.\"\"\"\n",
    "        self.window = window\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = pd.DataFrame()\n",
    "        df['residual'] = X\n",
    "        df['prior'] = df['residual'].shift(1)\n",
    "        df['mean'] = df['residual'].rolling(window=self.window).mean()\n",
    "        df['diff'] = df['residual'].diff().rolling(window=self.window).mean()\n",
    "        df = df.fillna(method='bfill')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "class FullModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, baseline, residual_model, steps=20):\n",
    "        \"\"\"Combine a baseline and residual model to predict any number of steps in the future.\"\"\"\n",
    "        \n",
    "        self.baseline = baseline\n",
    "        self.residual_model = residual_model\n",
    "        self.steps = steps\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.baseline.fit(X, y)\n",
    "        resd = y - self.baseline.predict(X)\n",
    "        self.residual_model.fit(resd.iloc[:-self.steps], resd.shift(-self.steps).dropna())\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_b = pd.Series(self.baseline.predict(X), index=X.index)\n",
    "        resd = X['Appliances'] - y_b\n",
    "        resd_pred = pd.Series(self.residual_model.predict(resd), index=X.index)\n",
    "        resd_pred = resd_pred.shift(self.steps)\n",
    "        y_pred = y_b + resd_pred\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# construct residual model\n",
    "resd_train = y_train - pipe.predict(df_train)\n",
    "residual_feats = ResidualFeatures(window=20)\n",
    "residual_model = Pipeline([('residual_features', residual_feats), ('regressor', LinearRegression())])\n",
    "    \n",
    "# construct and train full model\n",
    "full_model = FullModel(pipe, residual_model, steps=1)\n",
    "full_model.fit(df_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = full_model.predict(df_hourly)\n",
    "resd = energy - y_pred\n",
    "ind = resd.index > cutoff\n",
    "print(\"Test set R^2: {:g}\".format(r2_score(energy.loc[ind], y_pred.loc[ind])))\n",
    "plot_results(df_hourly, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, our baseline model is not great at predicting future energy use. However, we can still utilize our baseline model for anomaly detection. Our analysis will focus on the final residuals of our baseline model. If an observation deviates significantly from the baseline, it will be flagged. The plot below illustrates the distribution and autocorrelation for our final residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resd.hist(bins=50, density=True);\n",
    "plt.show()\n",
    "\n",
    "autocorrelation_plot(resd.dropna())\n",
    "plt.xlabel(\"Lag (hours)\")\n",
    "plt.xlim([0, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* What conclusion can you make from the autocorrelation plot?\n",
    "* What do you observe in the distribution of residuals?\n",
    "* What is a good basis to use when determining whether the magnitude of a deviation is large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## z-Score\n",
    "\n",
    "Since there is little temporal correlation with residual values, we can assume that the residuals are independently sampled from the same distribution. Given this probabilistic perspective, we can quantify the degree of anomaly to each observation *if* we know the distribution the residuals are being sampled from. If the distribution has one peak, there is a lower probability of observing values far from the peak. The z-score is a relative measure of how far away a value is from the mean, normalized by the standard deviation.\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma},\n",
    "$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation of distribution. The larger the magnitude of the z-score, the lower the probability of observing the value. Exact percentages can only be known if we know the distribution. When the distribution is a normal or Gaussian distribution given by\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2} \\right),\n",
    "$$\n",
    "\n",
    "68% of the values will reside within $z = \\pm 1$. 95% and 99.7% of the values will reside in an interval of $z = \\pm 2$ and $z = \\pm 3$, respectively.\n",
    "\n",
    "Strictly speaking, our distribution of our residuals are not normal but it has a single peak and not heavily skewed. The general idea of the greater the magnitude of the z-score the more anomalous the observation is still valid. Let's calculate the z-score for each residual and display the results. Since our distribution is slightly skewed towards positive values, we will use a different z-score cutoff whether the residual is negative or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (resd - resd.mean())/ resd.std()\n",
    "z.plot()\n",
    "pd.Series(3, index=resd.index).plot(color=\"r\")\n",
    "pd.Series(-2, index=resd.index).plot(color=\"r\")\n",
    "plt.ylabel(\"z-score\")\n",
    "plt.legend([\"residual\", \"z-score cutoff\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_anomalies(z, cutoff_lower=-2, cutoff_upper=2):\n",
    "    ind_lower = z < cutoff_lower\n",
    "    ind_upper = z > cutoff_upper\n",
    "    \n",
    "    return z[ind_lower | ind_upper]\n",
    "\n",
    "find_anomalies(z, cutoff_lower=-2, cutoff_upper=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* How should we decide the appropriate z-score cutoff? What are important ideas and consequences should we consider?\n",
    "* If we are concerned at identifying as much of the anomalies as possible, how should the z-score cutoff be set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling z-score\n",
    "\n",
    "The calculation of the z-score relied on the entire time series for calculating the mean and standard deviation. For anomaly detection with time series, we will usually be streaming observations and the entire series will not be available. Instead, we can calculate the z-score on a window of observations rather than the entire time history. Using a window has the advantage of \n",
    "\n",
    "1. Not having to hold in memory a large amount of data.\n",
    "2. Reflecting the fact that it is better to use recent values to judge whether an observation is anomalous.\n",
    "3. Being more adaptive to recent changes in the process.\n",
    "\n",
    "Let's modify the z-score calculation to only use a window of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_z_score(x, window=20):\n",
    "    rolling = x.rolling(window=window)\n",
    "    mean_roll = rolling.mean().shift(1) # shift to not include current value\n",
    "    std_roll = rolling.std().shift(1)\n",
    "    \n",
    "    return (x - mean_roll) / std_roll\n",
    "\n",
    "z_roll = rolling_z_score(resd, window=20)\n",
    "z_roll.plot()\n",
    "pd.Series(3, index=resd.index).plot(color=\"r\")\n",
    "pd.Series(-2, index=resd.index).plot(color=\"r\")\n",
    "plt.ylabel(\"z-score\")\n",
    "plt.legend([\"residual\", \"z-score cutoff\"]);\n",
    "\n",
    "find_anomalies(z_roll, cutoff_lower=-2, cutoff_upper=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Can you improve on the baseline model for the case study? Note, there are other recorded values in the data set such as the home temperature and humidity. These other time series may help improve the $R^2$ value of the model.\n",
    "1. Package an anomaly detector for our time series case study into a class. What should be some hyperparameters to the model? Note, starting with version 0.20, `scikit-learn` has the `OutlierMixin` class that your custom class could inherit form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2020 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
