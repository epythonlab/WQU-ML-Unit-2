{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/ML_Support_Vector_Machines.py append\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines are a popular class of machine learning models that were developed in the 1990s. They are capable of both linear and non-linear classification and can also be used for regression and anomaly/outlier detection. They work well for wide class of problems but are generally used for problems with small or medium sized data sets. In this notebook, we will start off with a simple classifier model and extend and improve it to ultimately arrive at what is referred to a support vector machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard margin classifier\n",
    "\n",
    "A hard margin classifier is a model that uses a hyperplane to completely separate two classes. A hyperplane is a subspace with one less dimension as the ambient space. For example, the hyperplane of a two dimensional space is a line and the hyperplane of a three dimensional space is a plane. It is helpful to consider each observation in our data set as existing in a $p$-dimensional space where $p$ is the number of features (columns) in our data. A hyperplane is simply a generalization of a plane in $p$-dimensional spaces. Take a look at the figure below, where we plot three such hyperplanes to separate two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(centers=[[1, 1], [-1, -1]], cluster_std=0.4, random_state=0)\n",
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr)\n",
    "plt.plot(x, -x+0.25, '--k')\n",
    "plt.plot(x, -0.25*x-0.3, 'r--')\n",
    "plt.plot(x, -1.5*x+1, 'b--')\n",
    "plt.xlim([-2, 2])\n",
    "plt.ylim([-2, 2])\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "* Of the three hyperplanes, which one should you choose to separate the two classes? What motivated your decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We not only want a classifier that will completely separate both classes but one that is situated an equal distance from the two classes. Such a classifier will likely be a better decision boundary for new data. Our model should not only seek for complete separation of our classes but also create the largest _margin_ between the two classes. To find the classifier that results in the largest margin, we need to first define the equation of a hyperplane and understand how to calculate distances. The equation for a $p$-dimensional hyperplane is\n",
    "\n",
    "$$ x \\cdot \\tilde{\\beta} + \\tilde{\\beta}_0 = 0. $$\n",
    "\n",
    "$\\tilde{\\beta}$ defines the hyperplane and the set of $x$ that satisfies the above equation lie on the plane. Note, both $x$ and $\\tilde{\\beta}$ are $p$-dimensional vectors. For our classifier, we require that all points or *vectors* are on the correct side of the dividing hyperplane. If we enforce the $||\\tilde{\\beta}|| = 1$, then the result of $||x \\cdot \\tilde{\\beta} + \\tilde{\\beta}_0||$ will be equal to the distance between the vector and the hyperplane. For our two classes, let's assign a value of $\\pm 1$, where $y_j = +1$ and $y_j = -1$ are observations above and below the hyperplane, respectively. Under this convention, we seek to find a hyperplane that will satisfy the following criterion for all observations\n",
    "\n",
    "$$ y_j(x_j \\cdot \\tilde{\\beta} + \\tilde{\\beta}_0) \\geq 0.$$ \n",
    "\n",
    "The value of the term inside the parenthesis will be positive for vectors located above the hyperplane and negative for vectors located below. Given our convention for the label values of $y_j \\pm 1$, the inequality will be satisfied so long as the hyperplane perfectly separates the two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the maximum margin\n",
    "\n",
    "For a given problem, there could be various hyperplanes that satisfy the above inequality; the three classifiers shown above all satisfy the inequality. We want a classifier the creates the largest possible margin; we will need to include the following constraint,\n",
    "\n",
    "$$ y_j(x_j \\cdot \\tilde{\\beta} + \\tilde{\\beta}_0) \\geq M,$$\n",
    "\n",
    "where $M$ is the size of the margin. Remember, the term $x_j \\cdot \\tilde{\\beta} + \\tilde{\\beta}_0$ is the distance between a vector and the hyperplane and we require all vectors are at least $M$ distance away. For simplicity, we will define $\\beta \\equiv \\tilde{\\beta}/M$ and $\\beta_0 \\equiv \\tilde{\\beta}_0/M$ which results in $\\| \\beta \\|_2 = 1/M$. To maximize, the margin, we need to minimize $\\| \\beta \\|_2$.  The hyperplane resulting in the largest margin can be solved by \n",
    "\n",
    "$$ \\min_{\\beta, \\beta_0} \\frac{1}{2} \\|\\beta \\|_2, $$\n",
    "\n",
    "with the constraint\n",
    "\n",
    "$$ y_j(x_j \\cdot \\beta + \\beta_0) \\geq 1.$$\n",
    "\n",
    "Let's train the hard margin classifier on the data previously displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, FloatSlider, fixed\n",
    "\n",
    "def plot_svc_interact(X, y):\n",
    "    def plotter(log_C=1):\n",
    "        clf = svm.SVC(C=10**log_C, kernel='linear')\n",
    "        clf.fit(X, y)\n",
    "    \n",
    "        beta = clf.coef_[0]\n",
    "        beta_0 = clf.intercept_\n",
    "        slope = -beta[0]/beta[1]\n",
    "        intercept = -beta_0/beta[1]\n",
    "       \n",
    "        x_max = np.ceil(np.abs(X).max())\n",
    "        x = np.linspace(-x_max, x_max, 100)\n",
    "        margin_bound_1 = 1/beta[1] + slope*x + intercept\n",
    "        margin_bound_2 = -1/beta[1] + slope*x + intercept\n",
    "\n",
    "        plt.plot(x, slope*x + intercept, 'k')\n",
    "        plt.fill_between(x, margin_bound_1, margin_bound_2, color='k', alpha=0.25, linewidth=0)\n",
    "        plt.scatter(*clf.support_vectors_.T, s=100, c='y')\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr)\n",
    "        plt.axis([-x_max, x_max, -x_max, x_max])\n",
    "\n",
    "    return plotter\n",
    "\n",
    "plot_svc_interact(X, y)(log_C=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure displays the largest margin possible without having vectors inside the margin. The figure highlights two vectors, the vectors that prevent the margin from expanding. These vectors are referred to as _support vectors_ because they support the margin structure. You can think of the margin boundaries as a wall or fence and the support vectors help maintain or support the structure. The support vectors are the only vectors in the training set that influences the choice of hyperplane. Changing the values of the other vectors will not affect the margin, so long as they still stay out of the margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* The choice of margin was only dictated by a few number of training data. Would you expect that this classifier to be prone to bias or variance error?\n",
    "* What preprocessing should we apply to our data to make the algorithm work best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft margin classifier\n",
    "\n",
    "Not all cases is it possible to completely linearly separable the two classes. We need to relax the constraint that there be no margin violations. The result is what is called the soft-margin classifier. As before, we still look to create the largest possible margin but the model will incur a penalty for vectors that reside in the margin or are on the wrong side of the hyperplane. Mathematically, we are going to construct a margin such that\n",
    "\n",
    "$$ \\min_{\\beta, \\beta_0} \\frac{1}{2}\\| \\beta \\|_2 + C \\sum_j \\zeta_j, $$\n",
    "\n",
    "with the constraints\n",
    "\n",
    "$$ y_j(x_j \\cdot \\beta + \\beta_0) \\geq (1 - \\zeta_j), $$\n",
    "\n",
    "$$ \\zeta_j \\geq 0.$$\n",
    "\n",
    "The severity of all the violations are controlled by the hyperparameter $C$ and the magnitude of the penalty for each vector is proportional to $\\zeta_j$. The objective function we want to minimize has two parts, one that seeks for the largest margin and another the aims to reduce penalties from margin violations. Our constrain is slightly different from the hard margin classifier as it needs to consider that vectors may reside inside the margin. \n",
    "\n",
    "Each vector will have its own value of $\\zeta$. If a vector does not reside inside the margin and is on the right side of the hyperplane, then $\\zeta=0$ and we have the constraint for our hard margin classifier. These vectors will contribute to the cost function we want to minimize. If a vector is inside the margin, then $\\zeta$ needs to be greater than 0 to still satisfy the constraint. If a vector lies on the hyperplane, then $x_j \\cdot \\beta + \\beta_0 = 0$ and $\\zeta $ must be at least equal to 1 to satisfy the constraint. If the vector is on the wrong side of the hyperplane, then $\\zeta$ for that vector needs to be greater than 1.\n",
    "\n",
    "Determining the hyperplane coefficients of the soft margin classifier involves solving a convex quadratic minimization with linear constraints. It can solved using quadratic programming solvers and the time complexity will be $O(np)$. The soft-margin classifier in `scikit-learn` is available using the `svm.LinearSVC` class.\n",
    "\n",
    "**Questions**\n",
    "* What other model had a cost function composed of two \"competing\" terms? Can you relate these terms to bias vs. variance?\n",
    "* $C$ is a hyperparameter we must tune. How does changing $C$ affect variance and the number of support vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The soft margin classifier uses the hinge loss function, named because it resembles a hinge. There is no loss so long as a threshold is not exceeded. Beyond the threshold, the loss ramps up linearly. See the figure below for an illustrations of a hinge loss function. Negative distance means the observation is on the wrong side of the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 2, 100)\n",
    "hinge_loss = lambda x: -(x-1) if x-1 < 0 else 0\n",
    "\n",
    "plt.plot(x, list(map(hinge_loss, x)))\n",
    "plt.xlabel(\"$y(x\\cdot\\\\beta + \\\\beta_0$)\")\n",
    "plt.ylabel('loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the soft margin classifier on a data set that is not completely linear separable. The interactive visualization allows you to modify the hyperparameter $C$. Consider the effect of increasing and decreasing $C$. Note, for tuning purposes, it's best to use a logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(centers=[[1, 1], [-1, -1]], cluster_std=1.5, random_state=0, n_samples=200)\n",
    "\n",
    "log_C_slider = FloatSlider(min=-4, max=2, step=0.25, value=0, description='$\\log(C)$')\n",
    "interact(plot_svc_interact(X, y), log_C=log_C_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels for non-linear classification\n",
    "\n",
    "Using a hyperplane to separate the two classes will have limited performance as most problems require a non-linear decision boundary. One approach to overcome this limitation is to engineer non-linear features using the original features. Essentially, we are projecting our data onto a higher dimensional space where a linear classifier will perform substantially better. Consider the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=200, noise=0.2, factor=0.25, random_state=0)\n",
    "plt.scatter(*X.T, c=y, cmap=plt.cm.bwr)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly cannot linearly separate the two classes. However, we can create a new feature $x_3 = \\sqrt{x_1^2 + x_2^2}$, the distance from the origin. With the new feature, we are projecting our data onto a higher dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_projection(X, y):\n",
    "    XX, YY = np.meshgrid(np.linspace(-1, 1, 20), np.linspace(-1, 1, 20))\n",
    "    ZZ = 0.6*np.ones((20, 20))\n",
    "    x_3 = (X[:, 0]**2 + X[:, 1]**2)**0.5\n",
    "    X_new = np.hstack((X, x_3.reshape(-1, 1)))\n",
    "\n",
    "    def plotter(elev=30, azim=30):\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes(projection='3d')\n",
    "        ax.scatter(*X_new.T, c=y, cmap=plt.cm.bwr)\n",
    "        ax.plot_surface(XX, YY, ZZ, alpha=0.2);\n",
    "        ax.view_init(elev, azim)\n",
    "        ax.set_xlabel('$x_1$')\n",
    "        ax.set_ylabel('$x_2$')\n",
    "        ax.set_zlabel('$x_3$')\n",
    "\n",
    "    return plotter\n",
    "\n",
    "interact(plot_projection(X, y), elev=(0, 360), azim=(0, 360));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this higher dimension, it is possible for a hyperplane to adequately divide the two classes. The resulting decision boundary on the original, lower dimensional space, is non-linear. Check out this [visualization](https://www.youtube.com/watch?v=3liCbRZPrZA) for another example of using projections to help the performance of the classifier.\n",
    "\n",
    "In the example, we introduced the new non-linear term directly into data set. However, the way the objective function is solved allows us to _indirectly_ applying the projection, using what is called the kernel trick. A kernel is a function that creates the implicit mapping/projection to a higher dimensional space. The advantage of using a kernel trick are:\n",
    "\n",
    "* No direct feature generation that increases the size of the data set.\n",
    "* We can readily swap out and try different kernels to see which one performs the best.\n",
    "\n",
    "With the kernel, we can now refer to our model as a support vector machine. More about the mathematically details about the kernel trick are explained at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choices of kernels\n",
    "\n",
    "There are several popular choices of kernels; they are the polynomial, sigmoid, and Gaussian radial basis function (RBF). In `scikit-learn`, the choice of kernel is controlled by the keyword argument `kernel`. The table below \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Kernel Name</th>\n",
    "<th>$K(x,x')$</th>\n",
    "<th>`kernel` keyword argument</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Linear Kernel</td>\n",
    "<td>$x \\cdot x'$</td>\n",
    "<td>`'linear'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$d$-th Degree Polynomial</td>\n",
    "<td>&nbsp; &nbsp; &nbsp;$(r + \\gamma\\ x \\cdot x')^d$    </td>\n",
    "<td>`poly'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Gaussian RBF</td>\n",
    "<td>&nbsp; $\\exp{(- \\gamma\\, \\|x - x' \\|_2)} $</td>\n",
    "<td>`'rbf'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Sigmoid</td>\n",
    "<td>&nbsp; &nbsp; &nbsp; $\\tanh(\\gamma\\, x \\cdot x' + r)$</td>\n",
    "<td>`'sigmoid'`</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Usually, the radial basis function kernel will perform the best and there's only one additional hyperparameter to tune, $\\gamma$. An interesting note is that using the radial basis function kernel is equivalent to projecting our vectors to an infinite dimensional space. It would not be possible to use the radial basis function kernel without the kernel trick.\n",
    "\n",
    "The following blocks of code create an interactive visualization where you can experiment with different kernels and hyperparameter values for a situation where a non-linear decision boundary is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X_train, X_test, y_train, y_test):\n",
    "    def plotter(kernel='linear', log_gamma=1, log_C=1, deg=1, coef0=1):\n",
    "        clf = svm.SVC(C=10**log_C, kernel=kernel, gamma=10**log_gamma, coef0=coef0, probability=True)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        X1, X2 = np.meshgrid(np.linspace(-2, 3), np.linspace(-2, 2))\n",
    "        y_proba = clf.predict_proba(np.hstack((X1.reshape(-1, 1), X2.reshape(-1, 1))))[:, 1]\n",
    "        plt.contourf(X1, X2, y_proba.reshape(50, 50), 16, cmap=plt.cm.bwr, alpha=0.75)\n",
    "        plt.colorbar()\n",
    "\n",
    "        accuracy = clf.score(X_test, y_test)\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='white', cmap=plt.cm.bwr)\n",
    "        plt.xlabel('$x_1$')\n",
    "        plt.ylabel('$x_2$')\n",
    "        plt.title('test set accuracy: {}'.format(accuracy));\n",
    "\n",
    "    return plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(400, noise=0.25, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "log_C_slider = FloatSlider(min=-4, max=4, step=0.25, value=0, description='$\\log(C)$')\n",
    "log_gamma_slider = FloatSlider(min=-3, max=2, step=0.01, value=0, description='$\\log(\\gamma$)')\n",
    "deg_slider = IntSlider(min=1, max=4, step=1, value=2, description='$d$')\n",
    "coef0_slider = FloatSlider(min=-100, max=100, step=0.1, value=0, description='$r$')\n",
    "\n",
    "interact(plot_decision_boundary(X_train, X_test, y_train, y_test),\n",
    "         log_C=log_C_slider,\n",
    "         log_gamma=log_gamma_slider, \n",
    "         kernel=['rbf', 'linear', 'sigmoid', 'poly'],\n",
    "         deg=deg_slider,\n",
    "         coef0=coef0_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with logistic regression\n",
    "\n",
    "The SVM models resembles that of logistic regression. They are both linear binary classifiers, if we ignore the kernelized version. The difference between the two methods is the cost function they use to determine model parameters. The logistic regression uses the log loss while SVM uses the hinge loss. Both functions are plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 4, 100)\n",
    "hinge_loss = lambda x: -(x-1) if x < 1 else 0\n",
    "log_loss = np.log(1+np.exp(-x))\n",
    "\n",
    "plt.plot(x, list(map(hinge_loss, x)))\n",
    "plt.plot(x, log_loss, '--r')\n",
    "plt.xlabel(\"$y(x \\cdot \\\\beta + \\\\beta_0)$\")\n",
    "plt.ylabel('loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two cost functions have the same limiting behavior. \n",
    "\n",
    "* If an observation is on the correct side of the hyperplane and very far away, large **positive** value of $y(x\\cdot\\beta + \\beta_0)$, it will have _nearly_ zero loss for the log loss and exactly zero for the hinge loss.\n",
    "\n",
    "* If an observation is on the wrong side of the hyperplane and very far away, large **negative** value of $y(x\\cdot\\beta + \\beta_0)$, the log loss penalty is linear with respect to the distance to the hyperplane.  The hinge loss penalty is always linear.\n",
    "\n",
    "The matching limiting behavior, as $z \\to \\pm \\infty$, can be observed in the equations for the loss functions,\n",
    "\n",
    "$$ C_\\text{log} = \\ln(1 + \\exp(-z), $$\n",
    "\n",
    "$$ C_\\text{hinge} = \\max(0, 1 - z) $$\n",
    "where\n",
    "$$ z = y(x\\cdot \\beta + \\beta_0).$$\n",
    "\n",
    "The difference occurs in the intermediate zone. SVM uses a threshold; if the observation is not inside the margin and on the right side of the hyperplane, there is no penalty. It does not matter how far away from the hyperplane the observation is located, so long as it still on correct side and not in the margin. This allows for the model to generalize better. For logistic regression, there will always be non-zero loss. Since every observation will have a cost, the model will need to \"satisfy\" each observation with regards to where to locate the hyperplane. Logistic regression will hurt the models ability to generalize. Note, a regularization term is often added to the logistic regression cost function, helping it generalize. Despite these differences, logistic regression and linear SVM often achieve similar results.\n",
    "\n",
    "Here are some things to consider when choosing which of the two models to use.\n",
    "\n",
    "* If calculating probabilities is important, use **logistic regression** as it is a probabilistic model.\n",
    "* If the data is sufficiently linearly separable, both models can be used but **SVM** may work better in the presence of outliers.\n",
    "* If the two classes are not linearly separable, use **SVM** with a kernel.\n",
    "* If there is a large number of observations, 50,000 - 100,000, and a small number of features, it is best to manually create new features and use **logistic regression** or **linear SVM**. Kernelized SVM is slow to train with large number of observations.\n",
    "\n",
    "The following visualization lets you use either a linear regression or SVM. You can control the separation of the clusters as well as the presence of an outlier. Notice how the SVM works better than linear regression when there is an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def plot_svc_vs_lr(cluster_std=0.8, log_C=1, model='logistic regression', outlier=False):\n",
    "    X, y = make_blobs(centers=[[1, 1], [-1, -1]], cluster_std=cluster_std, random_state=0)\n",
    "\n",
    "    if outlier:\n",
    "        X = np.vstack((X, [-1.5, 0.]))\n",
    "        y = np.hstack((y, [0]))\n",
    "\n",
    "    name_to_clf = {'logistic regression': LogisticRegression(C=10**log_C, solver='lbfgs'),\n",
    "              'SVM': svm.SVC(C=10**log_C, kernel='linear')}\n",
    "    \n",
    "    clf = name_to_clf[model]\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    beta = clf.coef_[0]\n",
    "    beta_0 = clf.intercept_\n",
    "    slope = -beta[0]/beta[1]\n",
    "    intercept = -beta_0/beta[1]\n",
    "       \n",
    "    x_max = np.ceil(np.abs(X).max())\n",
    "    x = np.linspace(-x_max, x_max, 100)\n",
    "\n",
    "    plt.plot(x, slope*x + intercept, 'k')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr)\n",
    "    plt.axis([-x_max, x_max, -x_max, x_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_C_slider = FloatSlider(min=-4, max=4, step=0.25, value=1, description='$\\log(C)$')\n",
    "cluster_std_slider = FloatSlider(min=0.2, max=1.0, step=0.05, value=0.8, description='cluster $\\sigma$')\n",
    "\n",
    "interact(plot_svc_vs_lr,\n",
    "         cluster_std=cluster_std_slider,\n",
    "         log_C=log_C_slider,\n",
    "         model=['logistic regression', 'SVM']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for regression\n",
    "\n",
    "SVMs can be also be used for regression with some modifications to the constrained optimization. Instead of constructing a margin that avoids penalties incurred by vectors residing inside the margin, we train a model that includes as many vectors *inside* the margin as possible. Now, vectors that are inside the margin carry no penalty but will incur one if they are outside of the margin. Similarly as before, the penalty ramps up linearly the farther away the vector is from the edge of the margin. Instead of a hinge loss, the SVM regressor uses a well-loss cost function, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.25\n",
    "x = np.linspace(-1, 1, 100)\n",
    "well_loss = list(map(lambda x: abs(x)-eps if abs(x) > eps else 0, x))\n",
    "square_loss = x**2\n",
    "\n",
    "plt.plot(x, well_loss)\n",
    "plt.plot(x, square_loss)\n",
    "plt.xlabel('distance from the center')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['well loss', 'square loss']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new optimization is\n",
    "$$ \\min_{\\beta, \\beta_0, \\zeta_j, \\zeta^*_j} \\frac{1}{2} \\|\\beta \\|_2 + C \\sum_j (\\zeta_j + \\zeta^*_j) $$\n",
    "with the following constraints\n",
    "$$ y_j - \\beta \\cdot x_j - \\beta_0 \\leq \\epsilon + \\zeta_i, $$\n",
    "$$ \\beta \\cdot x_j + \\beta_0 - y_j \\leq \\epsilon + \\zeta^*_i, $$\n",
    "$$ \\zeta_i, \\zeta^* \\geq 0. $$\n",
    "\n",
    "The optimization problem is very similar as before but now we have two $\\zeta$ values for each vector since our model will incur a penalty if a vector resides on either side of the margin. The hyperparameter $\\epsilon$ determines the thickness of the margin and $C$ acts as the same way as with SVM classifier. In `scikit-learn` the linear SVM regressor is accessed from `svm.LinearSVR` while the kernelized SVM is accessed via `svm.SVR`.\n",
    "\n",
    "The following interactive visualization allows you to see the effect of alternating $C$ and $\\epsilon$ on a data set with a linear behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svr_interact(X, y):\n",
    "    def plotter(epsilon=0.5, log_C=2):\n",
    "        rgr = svm.SVR(kernel='linear', epsilon=epsilon, C=10**log_C)\n",
    "        rgr.fit(X, y)\n",
    "    \n",
    "        y_pred = rgr.predict(X)\n",
    "        ind = np.abs(y - y_pred) >= epsilon\n",
    "\n",
    "        plt.scatter(X[ind], y[ind], s=100, color='y')\n",
    "        plt.scatter(X, y)\n",
    "        plt.fill_between(X.reshape(-1,), y_pred - epsilon,  y_pred + epsilon, alpha=0.25, color='k', linewidth=0)\n",
    "        plt.plot(X, y_pred, '-k')\n",
    "        plt.xlabel('$x$')\n",
    "        plt.ylabel('$y$')\n",
    "\n",
    "    return plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = 2*x + 1 + 0.5*np.random.randn(100)\n",
    "\n",
    "log_C_slider = FloatSlider(min=-3, max=1, step=0.05, value=-1, description='$\\log(C)$')\n",
    "epsilon_slider = FloatSlider(min=0.05, max=2, step=0.05, value=0.5, description='$\\epsilon$')\n",
    "interact(plot_svr_interact(x.reshape(-1, 1), y), log_C=log_C_slider, epsilon=epsilon_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Use `SVR` for the California housing data; you should experiment with using different kernels. What kernel works the best?\n",
    "1. Support vector regression (with a linear kernel) and linear regression are similar but use a different cost function; we compared both loss functions above. Given the loss functions, which model would you think will work better with the presence of outliers? Test out your answer by using both support vector regression and linear regression to fit a line through a data set that has an outlier.\n",
    "\n",
    "```python\n",
    "np.random.seed(0)\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = 2*x + 1 + 0.5*np.random.randn(100)\n",
    "\n",
    "# include outlier\n",
    "X = np.vstack((x.reshape(-1, 1), [-1]))\n",
    "y = np.hstack((y, [3]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Lagrangian dual formulation\n",
    "\n",
    "Instead of solving the originally formulated optimization problem, we can reformulate the problem to construct what is called a dual problem. The original formulation is referred to as the primal. We can use Lagrangian multipliers which transforms the constrained minimization problem into an unconstrained problem. Under certain conditions, the solution to solving the dual problem is the same as solving if we had solved the primal. These conditions are met with our original quadratic optimization with linear constraints. Thus, we can either solve the primal or dual problem and get the same result. The purpose of reformulating the problem will become more apparent in the next section.\n",
    "\n",
    "The dual formulation of the soft-margin classifier is\n",
    "$$ \\min_{\\alpha_j} \\frac{1}{2} \\sum_{j'} \\sum_{j} \\alpha_{j'} \\alpha_j y_j y_{j'} x_j \\cdot x_{j'} - \\sum_{j} \\alpha_j, $$\n",
    "subject to\n",
    "$$ \\alpha_j \\geq 0, $$\n",
    "\n",
    "$$ \\sum_j y_j \\alpha_j = 0, $$\n",
    "\n",
    "$$ 0 \\leq \\alpha_j < C. $$\n",
    "\n",
    "Once solved, the coefficients of the hyperplane are\n",
    "$$ \\beta = \\sum_j \\alpha_j y_j x_j.$$\n",
    "\n",
    "Here, $\\alpha_j$ are the Lagrangian multipliers. Only the vectors that violate the margin have a non-zero value for it's multiplier. Given the equation for calculating the hyperplane coefficients with the multipliers, it becomes clear that only vectors with a non-zero multiplier contribute to the construction of the hyperplane. The mathematics are in agreement to our earlier statement that only the support vectors decides the chosen hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: The kernel trick\n",
    "\n",
    "As discussed earlier, you can introduce new non-linear terms into your data set directly. For example, if you have two features $x_1$ and $x_2$, you can introduce polynomial terms such as $x_1^2$, $x_1x_2$, and $x_2^2$. The dual formulation that applies the mapping to generate non-linear features is\n",
    "\n",
    "$$ L_D = \\frac{1}{2} \\sum_{j'} \\sum_{j} \\alpha_{j'} \\alpha_j y_j y_{j'} h(x_j) \\cdot h(x_{j'}) - \\sum_{j} \\alpha_j, $$\n",
    "\n",
    "where $h(x_j)$ is a function that projects the original vectors to the new higher dimensional space. However, if we have a large set of features, the number of new features will become too many, even if we only include terms of degree 2. In the dual formulation, only the result of the dot product of the vectors matter. Instead of expanding the dimensions of our vectors and then taking the dot product, we can pose a function $K$ such that \n",
    "\n",
    "$$ K(x_j, x_{j'}) = h(x_{j})\\cdot h(x_{j'}).$$ \n",
    "\n",
    "This function is referred to as a kernel. The result of using a kernel on the dot product of the vectors in the original space is mathematically equivalent to explicitly transforming our vector and then taking the dot product. The kernel function is _indirectly_ applying the feature transformation and avoids the creating vectors of very large dimensions. The the advantage of solving the problem using the dual formulation is that it allows for the use of the kernel trick. With the kernel, we can now refer to our model as a support vector machine. The kernelized form of the equation we want to minimize is\n",
    "\n",
    "$$ L_D = \\frac{1}{2} \\sum_{j'} \\sum_{j} \\alpha_{j'} \\alpha_j y_j y_{j'} K(x_j, x_{j'}) - \\sum_{j} \\alpha_j, $$\n",
    "\n",
    "Solving for the hyperplane coefficients using the dual formulation is $O(n^2p)$ to $O(n^3p)$. The training time complexity does not scale well with increasing number of observations. Because of training time complexity of SVMs, they are not useful when working with large data set. There is no hard cutoff, but `scikit-learn` recommends against using a SVM with a data set of more than 100,000 samples. The class `svm.SVC` provides the kernelized form of the model, solved using the dual formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2020 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
