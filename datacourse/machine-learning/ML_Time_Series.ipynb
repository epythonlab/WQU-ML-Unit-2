{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/ML_Time_Series.py append\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "<!-- requirement: images/time_series_CV.png -->\n",
    "<!-- requirement: data/arima_model.pkl -->\n",
    "<!-- requirement: data/co2_weekly_mlo.txt -->\n",
    "\n",
    "A time series is a sequence of measurements of a variable made over time.  The usually application of machine learning to a time series is to use past behavior to make forecasts. Since the time series are usually continuous values, forecasting is a supervised regression problem. Time series differ from the \"standard\" regression problems studied earlier because observations are _usually_ not independent and the only piece of data we have is the signal itself. We want to take advantage of the temporal nature of the data without the knowledge of the forces that caused those values. The general approach when working with a time series is to\n",
    "\n",
    "1. Plot the time series; notice any overall trends and seasonality.\n",
    "1. Detrend the time series by removing drift and seasonality.\n",
    "1. Fit a baseline model and calculate the residuals.\n",
    "1. Analyze the resulting residuals and generate features from the residuals.\n",
    "1. Train a machine learning model to forecast/predict residuals and add back the baseline model.\n",
    "\n",
    "For this notebook, we will be analyzing the atmospheric carbon dioxide levels measured from the Mauna Loa Observatory in Hawaii. More information about the data can be found [here](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a time series\n",
    "\n",
    "We can model our time series as having three components,\n",
    "\n",
    "$$ y(t) = \\mathrm{drift} + \\mathrm{seasonal} + \\mathrm{noise}. $$\n",
    "\n",
    "The components are defined as\n",
    "\n",
    "1. **Drift**: An overall trend present in the time series. An example of a drift model is\n",
    "$$ y(t) = \\mu t. $$\n",
    "Other commonly applied drift models are quadratic and exponential.\n",
    "\n",
    "1. **Seasonality**: A periodic behavior existing in the time series. For a given frequency $f$, a common model is\n",
    "$$ y(t) = A\\sin(2\\pi ft) + B\\cos(2\\pi ft). $$\n",
    "\n",
    "1. **Noise**: The part of the time series remaining after removing drift and seasonality. It is the residual of a model containing drift and seasonality.\n",
    "\n",
    "Our approach will be to identify the first two terms to create a baseline model, leaving behind the residuals or noise. This [link](https://people.duke.edu/~rnau/whatuse.htm) provides a list of different transformations that are commonly applied when analyzing time series.\n",
    "\n",
    "** Questions**\n",
    "* What are some examples of drift in real time series?\n",
    "* What are some examples of seasonality in real time series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation of time series data\n",
    "\n",
    "Since observations are not independent and we want to use past data to predict future values, we need to apply slightly different approach when training and testing a machine learning model. Given the temporal nature of the data, we need to preserve order and have the training set occur prior to the test set. For cross-validation, two common methods are used, sliding and forward chaining.\n",
    "\n",
    "* **Sliding Window**: The model is trained with data in a fixed window size and tested with data in the following window of the same size. Then the window _slides_ where the previous test data becomes the training data and repeated for the number of chosen folds.\n",
    "\n",
    "* **Forward Chaining**: The model is _initially_ trained/tested with windows of the same size as the sliding window method. However, for each subsequent fold, the training window increases in size, encompassing both the previous training data and test data. The new test window once again follows the training window but stays the same length. \n",
    "\n",
    "![time_series_CV](images/time_series_CV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `scikit-learn`, the forward chaining method is available in `sklearn.model_selection.TimeSeriesSplit`. See below for an example of using forward chaining with `GridSearchCV`. See this [link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) for more info on the usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "regressor = Ridge()\n",
    "param_grid = {\"alpha\": np.logspace(-2, 2, 100)}\n",
    "ts_cv = TimeSeriesSplit(5) # 5-fold forward chaining\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=ts_cv, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary signal\n",
    "\n",
    "Ideally, the resulting time series of the residuals will be **stationary**. A stationary signal or process is one in which statistical values such as the mean do not change with time. For our purposes, we are concerned about the special case where the mean, variance, and autocorrelation (explained more later) are not a function of time. This special case is called weakly stationary. Transforming a time series into a stationary process is crucial for time series analysis because a large number of analysis tools assume the process is stationary. It is easy to predict future values if things like the mean and variance stay the same with time. Consider a time series where new values are dependent on the past time series value and a random, uncorrelated noise $\\epsilon_t$.\n",
    "\n",
    "$$\n",
    "y_t = \\rho y_{t-1} + \\epsilon_t.\n",
    "$$\n",
    "\n",
    "The parameter $\\rho$ scales the contribution of the past value. If $\\epsilon_t$ is uncorrelated and has mean zero, it is referred to as **white noise**. If the values are sampled from a normal distribution, the white noise is then called **white Gaussian noise**. The following visualization allows you to scale the contribution of past values by adjusting $\\rho$. Notice the signal is stationary when $\\rho < 1$ but is no longer stationary when $\\rho=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def plot_signal(rho=0):\n",
    "    n = 1000\n",
    "    np.random.seed(0)\n",
    "\n",
    "    eps = np.random.randn(n)\n",
    "    y = np.zeros(n)\n",
    "    y[0] = eps[0]\n",
    "    var = np.zeros(n)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        y[i] = rho*y[i-1] + eps[i]\n",
    "        var[i] = y[:i].var()\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.plot(y)\n",
    "    plt.ylabel('y')\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(var)\n",
    "    plt.ylabel('$\\sigma_y$')    \n",
    "    plt.plot(var)\n",
    "\n",
    "interact(plot_signal, rho=FloatSlider(min=0, max=1, value=0, step=0.01, description='$\\\\rho$'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case when $\\rho=1$ is called a one-dimensional [random walk](https://en.wikipedia.org/wiki/Random_walk). A random walk is a stochastic/random process that describes the location of an object from successive random steps, random in both direction and size. The equation $y_t = y_{t-1} + \\epsilon_t$ is a random walk because the position at time $t$ is some random distance from the previous location $y_{t-1}$. There have been extensive research on random walk processes since they occur in a wide range of subjects, from financial models to particle diffusion. There are two main consequences of having residuals as white noise.\n",
    "\n",
    "1. You cannot predict/forecast future values because what is left is uncorrelated noise.\n",
    "1. You have an adequate time series model since there is no signal left to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Drift\n",
    "\n",
    "Let's load the atmospheric CO2 data set using pandas and plot the time series. The data set has weekly measurements but there are some missing values, denoted by `-999.99`. We will need to replace those missing values and create timestamps from the date info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data set\n",
    "columns = ['year', 'month', 'day', 'decimal date', 'molfrac', 'days', '1 yr ago', '10 yrs ago', 'since 1880']\n",
    "df = pd.read_csv('data/co2_weekly_mlo.txt', sep='\\s+', header=None, names=columns, na_values=-999.99)\n",
    "\n",
    "# create timestamp indices\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.set_index('decimal date')\n",
    "\n",
    "# replace missing values\n",
    "df['molfrac'] = df['molfrac'].fillna(method='ffill')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO2 = df['molfrac']\n",
    "CO2.plot()\n",
    "plt.ylabel('CO2 ppm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "* What are some behaviors do you observe in the time series?\n",
    "* What model would you pose to remove the drift?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atmospheric CO2 levels have been consisting increasing at a slightly superlinear fashion. While the choice of drift is somewhat subjective, we will use a quadratic fit. The quadratic features will be provided from the `PolynomialFeatures` transformer. Let's create a simple model that only captures the drift; we will worry about the seasonality later. We will perform a train/test split at the year 2010 and define some functions and classes to help with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class IndexSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"Return indices of a data frame for use in other estimators.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        indices = df.index\n",
    "        return indices.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(df, cutoff, target):\n",
    "    \"\"\"Perform a train/test split on a data frame based on a cutoff date.\"\"\"\n",
    "    \n",
    "    ind = df.index < cutoff\n",
    "    \n",
    "    df_train = df.loc[ind]\n",
    "    df_test = df.loc[~ind]\n",
    "    y_train = df.loc[ind, target]\n",
    "    y_test = df.loc[~ind, target]\n",
    "    \n",
    "    return df_train, df_test, y_train, y_test\n",
    "\n",
    "def plot_results(df, y_pred):\n",
    "    \"\"\"Plot predicted results and residuals.\"\"\"\n",
    "    \n",
    "    CO2.plot();\n",
    "    plt.plot(list(df.index), y_pred, '-r');\n",
    "    plt.xlabel('year')\n",
    "    plt.ylabel('CO2 ppm')\n",
    "    plt.legend(['true', 'predicted']);\n",
    "    plt.show();\n",
    "\n",
    "    plt.plot(resd)\n",
    "    plt.xlabel('year')\n",
    "    plt.ylabel('residual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# perform train/test split\n",
    "cutoff = 2010\n",
    "df_train, df_test, y_train, y_test = ts_train_test_split(df, cutoff, 'molfrac')\n",
    "\n",
    "# construct and train pipeline\n",
    "time = IndexSelector()\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "lr = LinearRegression()\n",
    "pipe = Pipeline([('indices', time),\n",
    "                 ('drift', poly),\n",
    "                 ('regressor', lr)])\n",
    "pipe.fit(df_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = pipe.predict(df)\n",
    "resd = CO2 - y_pred\n",
    "print(\"Test set R^2: {:g}\".format(pipe.score(df_test, y_test)))\n",
    "plot_results(df, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals exhibit a periodic behavior; our next task is to remove the seasonal component of our data set. Atmospheric CO2 levels have a yearly cyclic behavior due to seasonal variations in the uptake of CO2 by vegetation. In this case, we may already know about this seasonal pattern, however, we need a systematic way to determine the dominant periodic behaviors in a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Seasonality\n",
    "\n",
    "Any signal can be represented as a linear superposition of sines and cosines of varying frequencies $f_n$ and amplitudes $A_n$ and $B_n$,\n",
    "\n",
    "$$ y(t) = \\sum_n \\left(A_n \\sin(2\\pi f_n t) + B_n\\cos(2 \\pi f_n t) \\right). $$\n",
    "\n",
    "The **Fourier transform** decomposes a signal into a set of frequencies, allowing for us to determine the dominant frequencies that make up a time series. We are transforming our signal in the time domain into the frequency domain. Since we will be working with discrete data; the signal is sampled at discrete points in time, we will use the **discrete Fourier transform**. For $N$ uniformly sampled time series $y_n$, the transform is defined as\n",
    "\n",
    "$$ Y_k = \\sum^{N-1}_{n=0} y_n e^{-\\frac{2\\pi i}{N} kn}, $$\n",
    "\n",
    "$$ Y_k = \\sum^{N-1}_{n=0} y_n \\left[\\cos\\left(\\frac{2\\pi i}{N} kn\\right) - i\\sin\\left(\\frac{2\\pi i}{N} kn\\right) \\right], $$\n",
    "\n",
    "and $i$ is the imaginary number. The term $Y_k$ is the Fourier transform value for a frequency of $k$ cycles in $N$ samples; it is a complex number that represents both the amplitude and phase for its respective sinusoidal component. The amplitude for the frequency $k/N$ is \n",
    "\n",
    "$$ |Y_k|/N = \\frac{\\sqrt{\\mathrm{Re}(Y_k)^2 + \\mathrm{Im}(Y_k)^2}}{N}. $$\n",
    "\n",
    "The most common algorithm used to compute the discrete Fourier transform is the fast Fourier transform (FFT). The algorithm makes use of matrix factorization to have a time complexity of $O(n\\log n)$ as opposed to the naive  $O(n^2)$ implementation. Note, the time series needs to be uniformly sampled. The `scipy.fftpack` provides the FFT algorithm. Let's use FFT to determine the contributed frequencies in the signal below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "\n",
    "def fft_plot(a=1, b=1, c=1, fourier=True):\n",
    "    np.random.seed(0)\n",
    "    N = 100\n",
    "    t_end = 4\n",
    "    t = np.linspace(0, t_end, N)\n",
    "    \n",
    "    y = a*np.cos(2*np.pi*t) + b*np.sin(4*2*np.pi*t) + c*np.cos(8*2*np.pi*t) + 0.2*np.random.randn(N)\n",
    "    Y = fftpack.fft(y)\n",
    "    f = np.linspace(0, N, N)/t_end\n",
    "    \n",
    "    if fourier:\n",
    "        plt.subplot(211)\n",
    "        plt.plot(t, y)\n",
    "        plt.xlim([0, 4])\n",
    "        plt.ylim([-4, 4])\n",
    "        plt.xlabel('time')\n",
    "        \n",
    "        plt.subplot(212)\n",
    "        plt.plot(f, np.abs(Y)/len(Y))\n",
    "        plt.ylim([0, 2])\n",
    "        plt.xlabel('number of cyles in full window')\n",
    "        plt.tight_layout()\n",
    "    else:\n",
    "        plt.plot(t, y)\n",
    "        plt.xlim([0, 4])\n",
    "        plt.ylim([-4, 4])\n",
    "        plt.xlabel('time')\n",
    "\n",
    "fft_plot(a=1, b=1, c=1, fourier=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From visual inspection, it is not apparent the frequencies that contribute to the signal but is derived from\n",
    "\n",
    "$$ y(t) = a\\cos(2\\pi t) + b\\sin(8\\pi t) + c\\cos(16\\pi t) + \\epsilon(t). $$\n",
    "\n",
    "The signal is composed of three sines/cosines at frequencies of 1, 4, and 8 and random uncorrelated noise $\\epsilon(t)$. The signal spans 4 time units and is sampled 25 times per unit of time. In the interactive visualization below, we display the signal and the resulting Fourier transform, allowing for the change in the amplitude of each of the three sinusoidal terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "interact(fft_plot, a=(0, 4, 0.1), b=(0, 4, 0.1), c=(0, 4, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interpretation of the Fourier transform plot is that it is a histogram/distribution of the frequencies that contribute to the signal. The resulting graph has three peaks; each peak corresponds to a dominant frequency present in the signal. Notice how increasing the amplitude of one of the sinusoidal terms in the signal results in a larger value for the respective frequency in the Fourier transform plot.\n",
    "\n",
    "The $x$-axis represents frequency, where the smallest non-zero frequency is equal to $1/t_{span}$ where $t_{span}$ is the size of the window or duration of the time series. The highest frequency is the inverse of the sampling rate.\n",
    "\n",
    "**Questions**\n",
    "* Are there any interesting features in the plot of the Fourier transform?\n",
    "* What would happen if the magnitude of the noise increases? Would it be difficult to derive insight form the decomposed signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Fourier transform of a real signal, no imaginary part, is symmetric about the center of the frequency range. The symmetric part is a result of _aliasing_, the effect of not differentiating two signals from each other. The discrete Fourier transform cannot measure the contribution of frequencies greater than the half of the inverse of the sampling rate, referred to as the Nyquist frequency,\n",
    "\n",
    "$$ f_N = \\frac{1}{2\\Delta t}, $$\n",
    "\n",
    "where $\\Delta t$ is the sampling rate. In the visualization below, we display the sampled values of two signals, one below the Nyquist frequency and its higher frequency alias. The different signals have the same sampled values but are derived from different frequencies. Notice how the signal in green is too fast to properly measure with our sampling rate. During the time before the signal is sampled again, the curve has gone up (or down) and down (or up) and reached its max (or min) value. Given our sampling frequency, we cannot distinguish sampled data from the green curve from that of the blue curve, they are aliases of one another. Because of the aliasing effect, it is customary to only display the Fourier transform for frequencies less than the Nyquist, only the first half of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatSlider\n",
    "\n",
    "def plot_alias(f=0.2, blue=True, green=True):\n",
    "    t = np.linspace(0, 10, 500)\n",
    "    t_sampled = np.arange(0, 11)\n",
    "    \n",
    "    if blue:\n",
    "        plt.plot(t, np.sin(2*np.pi*f*t), 'b')\n",
    "    if green:\n",
    "        plt.plot(t, -np.sin(2*np.pi*(1-f)*t), 'g')\n",
    "        \n",
    "    l, m, b = plt.stem(t_sampled, \n",
    "                       np.sin(2*np.pi*f*t_sampled), \n",
    "                       linefmt='r', \n",
    "                       markerfmt='ro',\n",
    "                       use_line_collection=True)\n",
    "    plt.setp(b, visible=False)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.xticks(t_sampled)\n",
    "    plt.legend([\"f={}\".format(f), \"f={}\".format(1-f), \"sampled signal\"])\n",
    "\n",
    "interact(plot_alias, f=FloatSlider(min=0, max=1.0, step=0.05, value=0.05, description='$f$'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the atmospheric CO2 data, let's formally identify the most dominant frequencies. We subtract the mean before computing the Fourier transform. If not, there would be a large value at zero frequency. The Fourier transform of the residuals is plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = fftpack.fft(resd-resd.mean())\n",
    "t_span = CO2.index[-1] - CO2.index[0]\n",
    "f = np.linspace(0, len(Y), len(Y))/t_span\n",
    "\n",
    "plt.plot(f[:len(Y)//2], np.abs(Y[:len(Y)//2])/len(Y));\n",
    "plt.xlabel('frequency (1/yr)')\n",
    "plt.ylabel('amplitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are no dominant frequencies greater than fives times a year. Let's zoom in for further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f[:200], np.abs(Y)[:200]);\n",
    "plt.xlabel('frequency (1/yr)')\n",
    "plt.ylabel('amplitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see two dominant frequencies occurring at once and twice a year. Our updated baseline model is now\n",
    "\n",
    "$$ y(t) = A + Bt + Ct^2 + D\\sin(2\\pi t) + E\\cos(2\\pi t) + F\\sin(4\\pi t) + G\\cos(4\\pi t), $$\n",
    "\n",
    "where $t$ in expressed in units of years. To incorporate the seasonal components, we will construct a custom transformer and use a combination of pipelines and feature unions to construct our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FourierComponents(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, freqs):\n",
    "        \"\"\"Create features based on sin(2*pi*f*t) and cos(2*pi*f*t).\"\"\"\n",
    "        self.freqs = freqs\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        Xt = np.zeros((X.shape[0], 2*len(self.freqs)))\n",
    "        \n",
    "        for i, f in enumerate(self.freqs):\n",
    "\n",
    "            Xt[:, 2*i]= np.cos(2*np.pi*f*X).reshape(-1)\n",
    "            Xt[:, 2*i + 1] = np.sin(2*np.pi*f*X).reshape(-1)\n",
    "    \n",
    "        return Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# construct and train pipeline\n",
    "fourier = FourierComponents([1, 2]) # annual and biannual frequencies\n",
    "union = FeatureUnion([('drift', poly), ('fourier', fourier)])\n",
    "baseline = Pipeline([('indices', time),\n",
    "                 ('union', union),\n",
    "                 ('regressor', lr)])\n",
    "baseline.fit(df_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = baseline.predict(df)\n",
    "resd = CO2 - y_pred\n",
    "print(\"Test set R^2: {:g}\".format(baseline.score(df_test, y_test)))\n",
    "plot_results(df, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, we have a baseline model that works well but the residuals do not appear to be completely stationary. Our analysis is not done, we can focus our attention on extracting any patterns in the resulting correlated noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* What, if any, behavior do you observe in the current baseline model's residuals?\n",
    "* Instead of using $y(t) = A\\cos(2\\pi ft) + B\\sin(2\\pi t)$, we could have used the equivalent $y(t) = k\\sin(2\\pi ft - \\phi). $ Why would the former be preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling noise\n",
    "\n",
    "We can improve on our analysis by modeling the noise, the residuals of our baseline model. Specifically, we want to measure the persistence of past values on the signal. In other words, past values of our times series are correlated to current values. We expect there to be some correlation to past values but the persistence should die off for further values in the past. The **autocorrelation** will give us a measure the persistence of past values; it is a measure of how well correlated a signal is with a lag copy of itself. Let's define some important mathematical values that are crucial for understanding the autocorrelation.\n",
    "\n",
    "* **Covariance**: A measure of _joint_ variability of two variables,\n",
    "$$ \\mathrm{cov}(X, Y) = E[(X - E[X])(Y - E[Y])] =  \\frac{1}{N} \\sum^{n}_{i=1}(x_i - E[X])(y_i - E[Y]). $$\n",
    "\n",
    "* **Variance**: A measure of the variability of a variable with _itself_; the special case of the covariance,\n",
    "$$ \\mathrm{var}(X) =  \\mathrm{cov}(X, X) = E[(X - E[X])^2] = \\frac{1}{N} \\sum^{n}_{i=1}(x_i - E[X])^2. $$ \n",
    "\n",
    "* **Standard Deviation**: The square root of the variance,\n",
    "$$ \\sigma_X = \\sqrt{\\mathrm{var}(X)}. $$\n",
    "\n",
    "* **Correlation**: The normalized covariance that ranges from -1 to 1,\n",
    "$$\\rho(X, Y) = \\frac{\\mathrm{cov}(X, Y)}{\\sigma_X \\sigma_Y}. $$ \n",
    "\n",
    "Three important values and meanings of the correlation coefficient are:\n",
    "\n",
    "1. If $\\rho(X, Y) = 1$, then the two variables are completely linear correlated; an increase in one corresponds to a linear increase of the other.\n",
    "1. If $\\rho(X, Y) = 0$, then the two variables are uncorrelated. Higher values of one variable does not necessarily correspond to higher or lower values of the other.\n",
    "1. If $\\rho(X, Y) = -1$, then the two variables are completely linear anti-correlated; an increase in one corresponds to a linear decrease of the other.\n",
    "\n",
    "With the correlation coefficient, we can now mathematically define and better understand the autocorrelation of a signal. The autocorrelation as a function of the duration of the lag is defined as\n",
    "\n",
    "$$ R(\\tau) =  \\frac{\\mathrm{cov}(y(t), y(t-\\tau))}{\\sigma_{y} \\sigma_{y}} = \\frac{\\gamma(\\tau)}{\\sigma^2_{y}} = \\rho(y(t), y(t-\\tau)), $$\n",
    "\n",
    "where $\\tau$ is the duration of the lag/delay and $\\gamma$ is the autocovariance function. Since we are working with discrete data, we can define the lag with respect to the number of time steps $k$,\n",
    "\n",
    "$$ R(k) = \\frac{\\gamma(k)}{\\sigma_{y}^2} = \\rho(y_t, y_{t-k}). $$\n",
    "\n",
    "Since the autocorrelation is a measure of how correlated a signal is with a delayed copy of itself, plotting the autocorrelation function will reveal to us how correlated past values are. The pandas function `autocorrelation_plot` plots the autocorrelation function of the curve and includes 95% and 99% confidence values of the zero-correlation hypothesis. The point of interest in the curve is at what lag value is there no more correlation. Such value is the characteristic time scale of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(resd)\n",
    "plt.xlabel('Lag (weeks)')\n",
    "plt.xlim([0, 1000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that values past 400 to 500 weeks are not correlated with current values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise based feature generation\n",
    "\n",
    "With the noise/residual of our time series, we can generate features based on past values for each time step. These features can be\n",
    "\n",
    "* Statistics of a window of past values, such as the mean and max. \n",
    "* One hot encoded features based on things such as the days of the week and holidays.\n",
    "* External features for each time step, for example, the value of the stock market.\n",
    "\n",
    "After determining the characteristic time scale of our process, we can incorporate the time scale when deciding how to best generate features. A common statistic to calculate is the moving average. For a times series, the moving average of a point in time is some average value calculated using a subset of past values. There are different types of moving averages but two common ones are:\n",
    "\n",
    "* **Rolling Window Average**: The average is calculated for a window of $k$ previous points.\n",
    "\n",
    "$$ MA_t = \\frac{1}{k} \\sum^{n}_{n-k} y_k. $$\n",
    "\n",
    "* **Exponential Moving Average**: All points are included in calculating the average but are weighted using an exponential decay. In other words, values further in the past contribute less to the moving average than recent points. A nice property of the exponential moving average is that the moving average value can be calculated with only the current time series value and the previous exponential moving average value.\n",
    "\n",
    "$$ EMA_t = \\alpha y_t + (1 - \\alpha) EMA_{t-1}, $$\n",
    "\n",
    "where $\\alpha$ ranges from 0 to 1 and scales the strength of the contribution of past values. The value of $\\alpha$ is related to the half-life of the weights, the time for the weights to drop half of their value,\n",
    "\n",
    "$$ \\alpha = 1 - \\exp\\left[-\\frac{\\ln(2)}{t_{1/2}}\\right], $$\n",
    "\n",
    "where $t_{1/2}$ is the half-life. Note, while we have discussed rolling window and exponential moving _averages_, other values can be calculated for other statistics. \n",
    "\n",
    "In the visualizations below, you can control the window size and half-life of the rolling window and exponentially weighted average of the residuals. Notice how applying moving averages smooths out the residuals. These moving averages are sometimes used to smooth out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_window(window=10):\n",
    "    series = pd.Series(resd, index=df.index)\n",
    "\n",
    "    rolling_window = series.rolling(window=window).mean()\n",
    "    series.plot(alpha=0.5)\n",
    "    rolling_window.plot(linewidth=2, color='k')\n",
    "    plt.title('rolling window')\n",
    "    plt.xlabel('year')\n",
    "    plt.ylabel('moving average')\n",
    "    \n",
    "interact(plot_rolling_window, window=(1, 200, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_exponential_weighted(half_life=100):\n",
    "    series = pd.Series(resd, index=df.index)\n",
    "    exponential_weighted = series.ewm(halflife=half_life).mean()\n",
    "    \n",
    "    series.plot(alpha=0.5)\n",
    "    exponential_weighted.plot(linewidth=2, color='k')\n",
    "    plt.title('exponential weighted')\n",
    "    plt.xlabel('year')\n",
    "    plt.ylabel('moving average')\n",
    "\n",
    "half_life_slider = FloatSlider(min=1, max=100, step=0.1, value=10, description=\"half-life\")\n",
    "interact(plot_exponential_weighted, half_life=half_life_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* How does one determine a good value to use for window size or half-life?\n",
    "* Considering computer memory, what moving average is better to use, rolling window average or exponential moving average?\n",
    "* How does increasing the half-life affect $\\alpha$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a baseline model and resulting residuals, our goal is to construct a model to predict atmospheric CO2 levels 20 weeks into the future. In other words, given the time series values we have currently measured at time $t$, we want to predict or forecast the value of the time series 20 time steps into the future since we sample the data weekly. For our approach, we will use the current time step residual, the prior residual, the rolling mean of the residual, and the rolling mean of the difference of the residual to predict the residual 20 weeks later. For the rolling window, we will choose a window size of 100 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window=100):\n",
    "        \"\"\"Generate features based on window statistics of past noise/residuals.\"\"\"\n",
    "        self.window = window\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = pd.DataFrame()\n",
    "        df['residual'] = pd.Series(X, index=X.index)\n",
    "        df['prior'] = df['residual'].shift(1)\n",
    "        df['mean'] = df['residual'].rolling(window=self.window).mean()\n",
    "        df['diff'] = df['residual'].diff().rolling(window=self.window).mean()\n",
    "        df = df.fillna(method='bfill')\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# create and train residual model\n",
    "resd_train = y_train - baseline.predict(df_train)\n",
    "residual_feats = ResidualFeatures(window=100)\n",
    "residual_model = Pipeline([('residual_features', residual_feats), ('regressor', LinearRegression())])\n",
    "residual_model.fit(resd_train.iloc[:-20], resd_train.shift(-20).dropna())\n",
    "\n",
    "# evaluate model\n",
    "resd_pred = residual_model.predict(resd) # prediction for all time steps\n",
    "resd_pred = pd.Series(resd_pred, index=df.index)\n",
    "resd_pred = resd_pred.shift(20).dropna() # shift predicted values to matching time step\n",
    "resd_pred_test = resd_pred.loc[resd_pred.index > 2010] # evaluate only on 2010 values\n",
    "print(\"Residual test set R^2: {:g}\".format(r2_score(resd.loc[resd.index > 2010], resd_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the residual model, we can combine both the baseline and residual model to make forecasts of atmospheric CO2 levels. It is best to create a custom estimator to encapsulate the process of combining both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin\n",
    "\n",
    "class FullModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, baseline, residual_model, steps=20):\n",
    "        \"\"\"Combine a baseline and residual model to predict any number of steps in the future.\"\"\"\n",
    "        \n",
    "        self.baseline = baseline\n",
    "        self.residual_model = residual_model\n",
    "        self.steps = steps\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.baseline.fit(X, y)\n",
    "        resd = y - self.baseline.predict(X)\n",
    "        self.residual_model.fit(resd.iloc[:-self.steps], resd.shift(-self.steps).dropna())\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_b = pd.Series(self.baseline.predict(X), index=X.index)\n",
    "        resd = X['molfrac'] - y_b\n",
    "        resd_pred = pd.Series(self.residual_model.predict(resd), index=X.index)\n",
    "        resd_pred = resd_pred.shift(self.steps)\n",
    "        y_pred = y_b + resd_pred\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "# construct and train full model\n",
    "full_model = FullModel(baseline, residual_model, steps=20)\n",
    "full_model.fit(df_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = full_model.predict(df)\n",
    "resd = CO2 - y_pred\n",
    "ind = resd.index > 2010\n",
    "print(\"Test set R^2: {:g}\".format(r2_score(CO2.loc[ind], y_pred.loc[ind])))\n",
    "plot_results(df, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model works really well at making predictions 20 weeks into the future. Let's plot the histogram and autocorrelation of the final residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "mu = resd.mean()\n",
    "sigma = resd.std(ddof=1)\n",
    "dist = norm(mu, sigma)\n",
    "x = np.linspace(-2, 2, 100)\n",
    "f = dist.pdf(x)\n",
    "\n",
    "resd.hist(bins=40, density=True)\n",
    "plt.plot(x, f, '-r', linewidth=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_plot(resd.dropna())\n",
    "plt.xlim([0, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are Gaussian and while arguably past values are still correlated, they are not as correlated as they were before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical time series models\n",
    "\n",
    "There are a class of statistically based models for time series. Most of these models are provided by the `statsmodels` Python package. Unfortunately, the API of models are different than `scikit-learn`. For this section, we will briefly discuss these models and demonstrate their usage in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive and moving average models\n",
    "\n",
    "The autoregressive (AR) model of order $p$ states that the current time series value is linearly dependent on the past $p$ values with some white noise,\n",
    "\n",
    "$$y_t = c + \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + ... \\alpha_p y_{t-p} + \\epsilon_t = c + \\sum^{p}_{p=1} \\alpha_p y_{t-p} + \\epsilon_t, $$\n",
    "\n",
    "where $\\alpha_p$ are the model parameters, $y_{t-p}$ are past time series values, $c$ is a constant, and $\\epsilon_t$ is white noise. The name autoregressive refers to the model parameters being solved by applying regression with the time series values themselves. Our previous illustration discussing stationary signals is an autoregressive model of order one as the current value is equal to the scaled prior value plus some noise. Autoregressive models are great at capturing the mean reversion and momentum in the time series since it is based on a window of past values.\n",
    "\n",
    "Another model is the moving average (MA) model. Despite similar names, the MA model and concept of moving averages are different and should not be confused. The MA model of order $q$ says that the time series is linearly dependent on current and past shock values or noise,\n",
    "\n",
    "$$y_t = c + \\epsilon_t + \\beta_1 \\epsilon_{t-1} + \\beta_2 \\epsilon_{t-2} + ... \\beta_q \\epsilon_{t-q} = c + \\sum^{q}_{q=1} \\beta_q \\epsilon_{t-q} + \\epsilon_t, $$\n",
    "\n",
    "where $\\beta_q$ are the model parameters. The MA model captures the persisting effect of shock events on future time series values. To get the capabilities of both models, AR and MA models are added, forming a more general time series model referred to as autoregressive and moving average (ARMA) model. The coefficients of the AR models are solved using a variety of methods such as linear least squares regression. MA coefficients are more computationally intensive to solve because shock values are not directly observed, requiring non-linear fitting algorithms. When using ARMA, the order of both AR and MA need to be specified and can be different.\n",
    "\n",
    "**Question**\n",
    "* How should one identify an appropriate value for the order of either AR and MA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate the AR model from `statsmodels` for forecasting the residuals of the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_train.date.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "\n",
    "# create and fit AR model\n",
    "lag = 200\n",
    "resd_train = y_train - baseline.predict(df_train)\n",
    "ar = AR(resd_train.values, dates=df_train['date'], freq='W')\n",
    "ar = ar.fit(maxlag=lag)\n",
    "resd_ar_train_pred = ar.predict(start=lag, end=len(df_train)-1)\n",
    "\n",
    "# plot training set results\n",
    "plt.plot(list(df_train.index), y_train - baseline.predict(df_train), alpha=0.5)\n",
    "plt.plot(list(df_train.index[lag:]), resd_ar_train_pred, 'r')\n",
    "plt.xlabel('year');\n",
    "plt.ylabel('residual')\n",
    "plt.legend(['true', 'predicted'])\n",
    "plt.show();\n",
    "\n",
    "# plot 20 step forecast of test set\n",
    "steps = 20\n",
    "resd_ar_test_pred = ar.predict(start=len(df_train), end=len(df_train) + steps - 1)\n",
    "plt.plot(range(1, steps + 1), y_test.iloc[:steps] - baseline.predict(df_test.iloc[:steps]))\n",
    "plt.plot(range(1, steps + 1), resd_ar_test_pred)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('residual')\n",
    "plt.legend(['true', 'predicted']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general syntax for using the models from `statsmodels` is passing the training data when instantiating the model, fitting the model by passing the number of terms to include, and finally calling the `predict` method with the number of steps into the future to forecast. The AR model was able to capture the trends, the ups and downs, of the residuals but under predicted the magnitude of those trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA\n",
    "\n",
    "The ARMA model only works for a stationary process. One method to arrive at a stationary process is to apply a difference transformation, $\\Delta y_t = y_t - y_{t-1}$. In our example of a random walk, the series was not stationary but the time series of the difference is stationary because it only depends on white Gaussian noise. The autoregressive integrated moving average (ARIMA) model is a general form of ARMA that applies differencing to the time series in the hopes of generating a stationary process. The ARIMA model is often written as $\\mathrm{ARIMA}(p, d, q)$, where\n",
    "* $p$: Number of terms to include in the AR model.\n",
    "* $d$: The degree of differencing, how many times differencing is applied to the series.\n",
    "* $q$: Number of terms to include in the MA model.\n",
    "\n",
    "Let's use the ARIMA model provided by the `statsmodels` package on the noise/residual of Mauna Loa data. Since the model takes a long time to fit, we have provided a pickle file of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# arima = ARIMA(resd_train.values, order=(20, 1, 5), dates=df_train['date'], freq='W')\n",
    "# arima = arima.fit()\n",
    "\n",
    "# load pretrained model\n",
    "with open('data/arima_model.pkl', 'rb') as f:\n",
    "    arima = pickle.load(f)\n",
    "\n",
    "# plot 20 step forecast of test set\n",
    "steps = 20\n",
    "resd_arima_test_pred, _, _ = arima.forecast(steps)\n",
    "plt.plot(range(1, steps + 1), resd_arima_test_pred)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('residual');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Incorporate more features into the residual model. Consider including more window statistics and external features such as financial data. Measure the performance in both the residual and full model.\n",
    "1. Since we have relatively small number of features, we were not worried about overfitting with the residual model using linear regression. However, overfitting becomes a problem with more features and more complicate models. Chose a different model than linear regression and tune the model's hyperparameters. You may need to use `TimeSeriesSplit` in conjunction with `GridSearchCV` to properly tune the model.\n",
    "1. Use the full model to predict atmospheric CO2 levels for the first 20 weeks of 2019. Check to see how well the model performs once data has been made available.\n",
    "1. Practice using the AR model available in `statsmodels` by generating a time series in the form of $y_t = \\rho y_{t-1} + \\epsilon_t$. Compare the fitted AR model coefficient(s) to the chosen value of $\\rho$. The fitted AR model coefficients are stored in the `params` attribute.\n",
    "1. Using either AR, MA, ARMA, or ARIMA for the residual model when forecasting atmospheric CO2 levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2020 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
